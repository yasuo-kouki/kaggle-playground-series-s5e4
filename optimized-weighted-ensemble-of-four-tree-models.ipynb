{"cells":[{"cell_type":"markdown","metadata":{},"source":["# üß† Overview\n","\n","In this notebook, we **compare four tree-based regression models** ‚Äî  \n","üå≥ **XGBoost**, üåø **LightGBM**, üê± **CatBoost**, and üå≤ **Random Forest** ‚Äî  \n","to evaluate their predictive performance on the same dataset.\n","\n","After evaluating each model individually,  \n","we construct an **ensemble model** that combines the predictions of all four algorithms.  \n","The **blending weights** are optimized using **Optuna**, aiming to achieve the **best overall performance**.\n","\n","---\n","\n","### üéØ Objectives\n","\n","The main goals of this notebook are to:\n","\n","- Analyze the **performance differences** among popular tree-based algorithms.  \n","- Demonstrate how **ensemble learning with optimized weights** can further enhance predictive accuracy.  \n","- Provide a **practical workflow** for combining models efficiently.\n","\n","---\n","\n","### üèÜ Result Summary\n","\n","The final **weighted ensemble** achieved a **score of 0.05555** on the evaluation metric (public leaderboard).  \n","This demonstrates that optimized blending can yield consistent improvements  \n","over individual tree-based learners, even when their standalone performances are similar.\n","\n","---\n","\n","üìà *By the end, we show that carefully tuned ensemble models can outperform individual learners ‚Äî  \n","highlighting the real-world power of **model blending** in predictive modeling.*"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:29:55.021761Z","iopub.status.busy":"2025-10-19T11:29:55.021496Z","iopub.status.idle":"2025-10-19T11:30:10.002641Z","shell.execute_reply":"2025-10-19T11:30:10.001554Z","shell.execute_reply.started":"2025-10-19T11:29:55.021739Z"},"trusted":true},"outputs":[],"source":["# ===============================\n","# üìö Library Imports\n","# ===============================\n","\n","# Basic libraries\n","import os\n","import numpy as np\n","import pandas as pd\n","\n","# Visualization\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import shap\n","\n","# Preprocessing\n","from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler, MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","\n","# Machine Learning Models\n","import xgboost as xgb\n","import lightgbm as lgb\n","import catboost as cb\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# Evaluation Metrics\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# Optimization\n","import optuna\n","\n","# Statistical tools\n","from scipy import stats\n","\n","# Model saving & loading\n","import joblib\n","\n","# Display input file paths (Kaggle environment)\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:10.004413Z","iopub.status.busy":"2025-10-19T11:30:10.003600Z","iopub.status.idle":"2025-10-19T11:30:11.410370Z","shell.execute_reply":"2025-10-19T11:30:11.409497Z","shell.execute_reply.started":"2025-10-19T11:30:10.004382Z"},"trusted":true},"outputs":[],"source":["# üìÇ Data Loading / „Éá„Éº„ÇøË™≠„ÅøËæº„Åø\n","train = pd.read_csv(\"/kaggle/input/playground-series-s5e10/train.csv\")\n","predict = pd.read_csv(\"/kaggle/input/playground-series-s5e10/test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:11.412303Z","iopub.status.busy":"2025-10-19T11:30:11.412013Z","iopub.status.idle":"2025-10-19T11:30:11.445443Z","shell.execute_reply":"2025-10-19T11:30:11.444711Z","shell.execute_reply.started":"2025-10-19T11:30:11.412282Z"},"trusted":true},"outputs":[],"source":["train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## üìä Data Overview\n","\n","We begin by **exploring the dataset** to understand its structure and key characteristics.  \n","Using `info()` and `describe()`, we examine:\n","\n","- **Data types** of each feature  \n","- **Missing values** (if any)  \n","- **Summary statistics** such as mean, standard deviation, and range  \n","\n","Additionally, a **correlation heatmap** is visualized to identify potential relationships between features.\n","\n","---\n","\n","‚úÖ This step ensures that the dataset is **clean**, **well-structured**, and **free from strong multicollinearity** \n","providing a reliable foundation for model training.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:11.446555Z","iopub.status.busy":"2025-10-19T11:30:11.446313Z","iopub.status.idle":"2025-10-19T11:30:11.579131Z","shell.execute_reply":"2025-10-19T11:30:11.578255Z","shell.execute_reply.started":"2025-10-19T11:30:11.446536Z"},"trusted":true},"outputs":[],"source":["# Display basic information about the training dataset / Â≠¶Áøí„Éá„Éº„Çø„ÅÆÂü∫Êú¨ÊÉÖÂ†±„ÇíË°®Á§∫\n","train.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:11.580549Z","iopub.status.busy":"2025-10-19T11:30:11.580113Z","iopub.status.idle":"2025-10-19T11:30:11.700708Z","shell.execute_reply":"2025-10-19T11:30:11.699805Z","shell.execute_reply.started":"2025-10-19T11:30:11.580522Z"},"trusted":true},"outputs":[],"source":["train.describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:11.702123Z","iopub.status.busy":"2025-10-19T11:30:11.701651Z","iopub.status.idle":"2025-10-19T11:30:12.132337Z","shell.execute_reply":"2025-10-19T11:30:12.131382Z","shell.execute_reply.started":"2025-10-19T11:30:11.702096Z"},"trusted":true},"outputs":[],"source":["def plot_correlation_heatmap(df, figsize=(12, 8)):\n","    \"\"\"\n","    Display the correlation between all numerical features as a heatmap.\n","    \"\"\"\n","    numeric_cols = df.select_dtypes(include=[np.number]).columns\n","    corr_matrix = df[numeric_cols].corr()\n","    \n","    plt.figure(figsize=figsize)\n","    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n","                center=0, square=True, linewidths=1)\n","    plt.title('Feature Correlation Heatmap', fontsize=16)\n","    plt.tight_layout()\n","    plt.show()\n","    \n","    return corr_matrix\n","\n","# Display the correlation heatmap of all features in the training data\n","corr_matrix = plot_correlation_heatmap(train.drop(columns=['id', 'accident_risk']))"]},{"cell_type":"markdown","metadata":{},"source":["## üß© Feature Engineering & One-Hot Encoding\n","\n","We enhance the dataset by **creating additional meaningful features** and transforming categorical variables into a numerical format suitable for model training.\n","\n","- **Boolean columns** are converted into integer values (`0` and `1`).  \n","- **One-hot encoding** is applied to categorical features to ensure full compatibility with all tree-based models.  \n","- These transformations help models capture **nonlinear relationships** and handle **categorical diversity** effectively.\n","\n","---\n","\n","üß† *Feature engineering plays a crucial role in improving model performance by providing richer, more informative inputs for learning.*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:12.133636Z","iopub.status.busy":"2025-10-19T11:30:12.133361Z","iopub.status.idle":"2025-10-19T11:30:12.148574Z","shell.execute_reply":"2025-10-19T11:30:12.147669Z","shell.execute_reply.started":"2025-10-19T11:30:12.133607Z"},"trusted":true},"outputs":[],"source":["def create_features(df):\n","    \n","    df['curavture_accidents'] = df['curvature'] * df['num_reported_accidents']\n","    \n","    return df\n","\n","train = create_features(train)\n","predict = create_features(predict)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:12.149928Z","iopub.status.busy":"2025-10-19T11:30:12.149636Z","iopub.status.idle":"2025-10-19T11:30:12.403759Z","shell.execute_reply":"2025-10-19T11:30:12.402911Z","shell.execute_reply.started":"2025-10-19T11:30:12.149907Z"},"trusted":true},"outputs":[],"source":["# One-Hot Encoding\n","def one_hot_encode(df, columns):\n","    df = pd.get_dummies(df, columns=columns, drop_first=False)\n","    return df\n","\n","# categorical variables\n","encode_columns = ['road_type', 'lighting', 'weather', 'time_of_day']\n","\n","train = one_hot_encode(train, encode_columns)\n","predict = one_hot_encode(predict, encode_columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:12.406405Z","iopub.status.busy":"2025-10-19T11:30:12.406122Z","iopub.status.idle":"2025-10-19T11:30:12.452710Z","shell.execute_reply":"2025-10-19T11:30:12.451830Z","shell.execute_reply.started":"2025-10-19T11:30:12.406387Z"},"trusted":true},"outputs":[],"source":["# Convert boolean columns\n","\n","def bool_to_int(df):\n","    bool_columns = df.select_dtypes(include='bool').columns\n","    for col in bool_columns:\n","        df[col] = df[col].astype(int)\n","    return df\n","\n","train = bool_to_int(train)\n","predict = bool_to_int(predict)"]},{"cell_type":"markdown","metadata":{},"source":["## ‚öñÔ∏è Data Scaling and Train-Test Split\n","\n","To ensure **consistent feature scaling** across all variables, we apply **standardization** to numerical features.  \n","This process helps stabilize training and improves model convergence, especially for algorithms sensitive to feature magnitude.\n","\n","After scaling, the dataset is **split into training and validation sets**, allowing for an **unbiased comparison** of model performance under identical conditions.\n","\n","---\n","\n","üìè *Proper scaling and data splitting ensure fair and reliable evaluation across all tree-based models.*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:12.454431Z","iopub.status.busy":"2025-10-19T11:30:12.454090Z","iopub.status.idle":"2025-10-19T11:30:12.561347Z","shell.execute_reply":"2025-10-19T11:30:12.560419Z","shell.execute_reply.started":"2025-10-19T11:30:12.454405Z"},"trusted":true},"outputs":[],"source":["# Since the residual errors are large, use RobustScaler, which is less sensitive to outliers\n","def robust_scale(df):\n","    scaler = RobustScaler()\n","\n","    numeric_columns = ['curvature','curavture_accidents']\n","\n","    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n","    return df\n","\n","train = robust_scale(train)\n","predict = robust_scale(predict)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:12.562650Z","iopub.status.busy":"2025-10-19T11:30:12.562388Z","iopub.status.idle":"2025-10-19T11:30:12.950971Z","shell.execute_reply":"2025-10-19T11:30:12.949919Z","shell.execute_reply.started":"2025-10-19T11:30:12.562631Z"},"trusted":true},"outputs":[],"source":["def split_data(df,test_size=0.2,random_state=42):\n","    X = df.drop(columns=['id','accident_risk'])\n","    y = df['accident_risk']\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n","    return X_train, X_test, y_train, y_test\n","\n","X_train, X_test, y_train, y_test = split_data(train)\n","\n","predict_X = predict.copy()\n","predict_X = predict_X.drop(columns=['id'])"]},{"cell_type":"markdown","metadata":{},"source":["# üå≤ Model Training\n","\n","We train **four optimized tree-based models** ‚Äî  \n","**XGBoost**, **LightGBM**, **CatBoost**, and **Random Forest** ‚Äî on the same dataset to evaluate their predictive performance.\n","\n","Each model is built using **hyperparameters optimized via tuning techniques such as Optuna**, ensuring both **strong generalization** and a **fair comparison** across all algorithms.\n","\n","---\n","\n","üöÄ *This step establishes a solid foundation for model evaluation and highlights how hyperparameter optimization can enhance accuracy and consistency.*\n"]},{"cell_type":"markdown","metadata":{},"source":["## XGBOOST"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:12.952053Z","iopub.status.busy":"2025-10-19T11:30:12.951792Z","iopub.status.idle":"2025-10-19T11:30:19.115953Z","shell.execute_reply":"2025-10-19T11:30:19.115057Z","shell.execute_reply.started":"2025-10-19T11:30:12.952033Z"},"trusted":true},"outputs":[],"source":["# Build an optimized XGBoost regression model\n","def build_xgboost_model(\n","    n_estimators=431,\n","    max_depth=10,\n","    learning_rate=0.08116012956704159,\n","    subsample=0.671117833063129,\n","    colsample_bytree=0.928949134207643,\n","    gamma=0.02035246955467515,\n","    min_child_weight=3,\n","    random_state=42\n","):\n","    \"\"\"Build an optimized XGBoost regression model\"\"\"\n","    model = xgb.XGBRegressor(\n","        objective='reg:squarederror',  # Loss function suitable for regression (RMSE)\n","        n_estimators=n_estimators,      # Number of boosting trees\n","        max_depth=max_depth,            # Maximum tree depth\n","        learning_rate=learning_rate,    # Step size shrinkage used to prevent overfitting\n","        subsample=subsample,            # Fraction of samples used for each tree\n","        colsample_bytree=colsample_bytree,  # Fraction of features used per tree\n","        gamma=gamma,                    # Minimum loss reduction required to make a further partition\n","        min_child_weight=min_child_weight,  # Minimum sum of instance weight needed in a child\n","        random_state=random_state       # Random seed for reproducibility\n","    )\n","    return model\n","\n","\n","# Build the model\n","xgb_model = build_xgboost_model()\n","\n","# Train the model\n","xgb_model.fit(X_train, y_train)\n"]},{"cell_type":"markdown","metadata":{},"source":["## CATBOOST"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:19.123226Z","iopub.status.busy":"2025-10-19T11:30:19.122922Z","iopub.status.idle":"2025-10-19T11:30:37.169685Z","shell.execute_reply":"2025-10-19T11:30:37.168966Z","shell.execute_reply.started":"2025-10-19T11:30:19.123199Z"},"trusted":true},"outputs":[],"source":["# Build an optimized CatBoost model (for regression tasks)\n","def build_catboost_model(\n","    iterations=474,\n","    depth=8,\n","    learning_rate=0.10105022929242771,\n","    l2_leaf_reg=3.905251076718455,\n","    border_count=106,\n","    random_seed=42,\n","    verbose=100\n","):\n","    \"\"\"Build an optimized CatBoost regression model\"\"\"\n","    model = cb.CatBoostRegressor(\n","        iterations=iterations,           # Number of boosting iterations (trees)\n","        depth=depth,                     # Maximum depth of the trees\n","        learning_rate=learning_rate,     # Learning rate (controls step size)\n","        l2_leaf_reg=l2_leaf_reg,         # L2 regularization term (helps prevent overfitting)\n","        border_count=border_count,       # Number of splits for numerical features\n","        loss_function='RMSE',            # Loss function suitable for regression\n","        random_seed=random_seed,         # Random seed for reproducibility\n","        verbose=verbose                  # Logging frequency during training\n","    )\n","    return model\n","\n","\n","# Build the model\n","cat_model = build_catboost_model()\n","\n","# Train the model\n","cat_model.fit(X_train, y_train)\n"]},{"cell_type":"markdown","metadata":{},"source":["## LIGHTGBM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:37.170835Z","iopub.status.busy":"2025-10-19T11:30:37.170541Z","iopub.status.idle":"2025-10-19T11:30:46.727606Z","shell.execute_reply":"2025-10-19T11:30:46.726656Z","shell.execute_reply.started":"2025-10-19T11:30:37.170803Z"},"trusted":true},"outputs":[],"source":["# Build an optimized LightGBM regression model\n","def build_lightgbm_model(\n","    n_estimators=381,\n","    max_depth=7,\n","    learning_rate=0.039237697728597476,\n","    subsample=0.790446416831238,\n","    colsample_bytree=0.9592823831701073,\n","    num_leaves=119,\n","    min_child_samples=29,\n","    random_state=42\n","):\n","    \"\"\"Build an optimized LightGBM regression model\"\"\"\n","    model = lgb.LGBMRegressor(\n","        objective='regression',          # For regression tasks\n","        n_estimators=n_estimators,       # Number of boosting trees\n","        max_depth=max_depth,             # Maximum depth of each tree\n","        learning_rate=learning_rate,     # Learning rate\n","        subsample=subsample,             # Fraction of samples used for each tree\n","        colsample_bytree=colsample_bytree,  # Fraction of features used per tree\n","        num_leaves=num_leaves,           # Maximum number of leaves per tree\n","        min_child_samples=min_child_samples, # Minimum number of samples per leaf\n","        random_state=random_state,       # Random seed for reproducibility\n","        verbose=-1                       # Suppress training logs\n","    )\n","    return model\n","\n","\n","# Build the model\n","lgb_model = build_lightgbm_model()\n","\n","# Train the model\n","lgb_model.fit(X_train, y_train)\n"]},{"cell_type":"markdown","metadata":{},"source":["## RANDOM_FOREST"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:30:46.728783Z","iopub.status.busy":"2025-10-19T11:30:46.728514Z","iopub.status.idle":"2025-10-19T11:32:20.597847Z","shell.execute_reply":"2025-10-19T11:32:20.596854Z","shell.execute_reply.started":"2025-10-19T11:30:46.728755Z"},"trusted":true},"outputs":[],"source":["# Build an optimized Random Forest regression model\n","def build_random_forest_model(\n","    n_estimators=195,\n","    max_depth=11,\n","    min_samples_split=10,\n","    min_samples_leaf=2,\n","    max_features=0.9342067158084462,\n","    bootstrap=True,\n","    random_state=42\n","):\n","    \"\"\"Build an optimized Random Forest regression model\"\"\"\n","    model = RandomForestRegressor(\n","        n_estimators=n_estimators,       # Number of trees in the forest\n","        max_depth=max_depth,             # Maximum depth of each decision tree\n","        min_samples_split=min_samples_split, # Minimum number of samples required to split a node\n","        min_samples_leaf=min_samples_leaf,   # Minimum number of samples required at a leaf node\n","        max_features=max_features,       # Fraction of features considered for each split\n","        bootstrap=bootstrap,             # Whether bootstrap samples are used when building trees\n","        random_state=random_state,       # Random seed for reproducibility\n","        n_jobs=-1                        # Use all CPU cores for parallel processing\n","    )\n","    return model\n","\n","\n","# Build the model\n","rf_model = build_random_forest_model()\n","\n","# Train the model\n","rf_model.fit(X_train, y_train)\n"]},{"cell_type":"markdown","metadata":{},"source":["# üìä Model Predictions and Performance Evaluation\n","\n","We generate **predictions** for each trained model ‚Äî  \n","**XGBoost**, **LightGBM**, **CatBoost**, and **Random Forest** ‚Äî and evaluate their performance on the **test dataset** using key regression metrics:  \n","**Mean Squared Error (MSE)**, **Mean Absolute Error (MAE)**, and **R¬≤ score**.\n","\n","This step helps to **quantify each model‚Äôs predictive capability** and identify their respective **strengths and weaknesses** before constructing the final **ensemble model**.\n","\n","---\n","\n","üìà *By comparing these metrics side by side, we gain valuable insights into which algorithm performs best individually ‚Äî and where blending may provide further improvements.*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:32:20.599136Z","iopub.status.busy":"2025-10-19T11:32:20.598807Z","iopub.status.idle":"2025-10-19T11:32:23.691400Z","shell.execute_reply":"2025-10-19T11:32:23.690576Z","shell.execute_reply.started":"2025-10-19T11:32:20.599111Z"},"trusted":true},"outputs":[],"source":["# prediction on Test Data\n","xgb_pred = xgb_model.predict(X_test)\n","cat_pred = cat_model.predict(X_test)\n","lgb_pred = lgb_model.predict(X_test)\n","rf_pred = rf_model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:32:23.692540Z","iopub.status.busy":"2025-10-19T11:32:23.692247Z","iopub.status.idle":"2025-10-19T11:32:23.697683Z","shell.execute_reply":"2025-10-19T11:32:23.696837Z","shell.execute_reply.started":"2025-10-19T11:32:23.692512Z"},"trusted":true},"outputs":[],"source":["def evalute_metrics(y_true, y_pred):\n","    mse = mean_squared_error(y_true, y_pred)\n","    mae = mean_absolute_error(y_true, y_pred)\n","    r2 = r2_score(y_true, y_pred)\n","\n","    results = pd.DataFrame([{\n","        'MSE': mse,\n","        'MAE': mae,\n","        'R2': r2\n","    }])\n","\n","    return results"]},{"cell_type":"markdown","metadata":{},"source":["# üìù Interpretation\n","\n","All four **tree-based models** achieve **very similar performance**, with only **minor variations** across MSE, MAE, and R¬≤ values.\n","\n","Among them, **LightGBM** slightly outperforms the others ‚Äî  \n","showing the **lowest MSE** and **highest R¬≤**, which indicates **marginally better predictive accuracy and generalization** compared to XGBoost, CatBoost, and Random Forest.\n","\n","---\n","\n","üìä *This consistency among models suggests that the dataset is well-structured and all algorithms are effectively capturing its underlying patterns.*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:38:59.681390Z","iopub.status.busy":"2025-10-19T11:38:59.680569Z","iopub.status.idle":"2025-10-19T11:38:59.710856Z","shell.execute_reply":"2025-10-19T11:38:59.709950Z","shell.execute_reply.started":"2025-10-19T11:38:59.681363Z"},"trusted":true},"outputs":[],"source":["xgb_results = evalute_metrics(y_test, xgb_pred).assign(Model=\"XGBoost\")\n","cat_results = evalute_metrics(y_test, cat_pred).assign(Model=\"CatBoost\")\n","lgb_results = evalute_metrics(y_test, lgb_pred).assign(Model=\"LightGBM\")\n","rf_results  = evalute_metrics(y_test, rf_pred).assign(Model=\"RandomForest\")\n","\n","# ÁµêÊûú„ÇíÁµêÂêà„Åó„Å¶Ë¶ã„ÇÑ„Åô„ÅèË°®Á§∫\n","results = pd.concat([xgb_results, cat_results, lgb_results, rf_results], ignore_index=True)\n","results = results[['Model', 'MSE', 'MAE', 'R2']]\n","\n","display(results.sort_values('MSE'))"]},{"cell_type":"markdown","metadata":{},"source":["# üß†Ensemble Learning with Optimized Weights (Optuna)\n","\n","To enhance prediction performance, we perform a **weighted ensemble** of four tree-based models:  \n","**XGBoost**, **LightGBM**, **CatBoost**, and **Random Forest**.\n","\n","We use **Optuna** to automatically search for the optimal combination of weights that minimizes the **Mean Squared Error (MSE)** on the validation set.  \n","Each model‚Äôs prediction is linearly combined according to the optimized weights.\n","\n","---\n","\n","#### üîçOptimization Process\n","1. Define search space for each model‚Äôs weight (`0.0‚Äì1.0`).\n","2. Normalize weights so that their total equals 1.\n","3. Compute the weighted average of predictions.\n","4. Evaluate the result with **MSE**.\n","5. Repeat the process with Optuna‚Äôs **TPE sampler** to find the best combination.\n","\n","---\n","\n","#### üìäFinal Ensemble Output\n","The final ensemble uses the best weight combination found by Optuna to produce the final prediction, and the performance is evaluated with **RMSE**.\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:32:51.008890Z","iopub.status.busy":"2025-10-19T11:32:51.008583Z","iopub.status.idle":"2025-10-19T11:33:12.123300Z","shell.execute_reply":"2025-10-19T11:33:12.122422Z","shell.execute_reply.started":"2025-10-19T11:32:51.008868Z"},"trusted":true},"outputs":[],"source":["import os\n","import optuna\n","import numpy as np\n","from sklearn.metrics import mean_squared_error\n","\n","# --- Optimization function ---\n","def optimize_weight(trial):\n","    w_xgb = trial.suggest_float('xgb_weight', 0.0, 1.0)\n","    w_lgb = trial.suggest_float('lgb_weight', 0.0, 1.0)\n","    w_rf  = trial.suggest_float('rf_weight', 0.0, 1.0)\n","    w_cat = trial.suggest_float('cat_weight', 0.0, 1.0)\n","\n","    total_weight = w_xgb + w_lgb + w_rf + w_cat\n","    if total_weight == 0:\n","        return np.inf  # Invalid case\n","\n","    # Normalize weights\n","    w_xgb /= total_weight\n","    w_lgb /= total_weight\n","    w_rf  /= total_weight\n","    w_cat /= total_weight\n","\n","    # Weighted ensemble prediction\n","    final_pred = (\n","        w_xgb * xgb_pred +\n","        w_lgb * lgb_pred +\n","        w_rf  * rf_pred +\n","        w_cat * cat_pred\n","    )\n","\n","    mse = mean_squared_error(y_test, final_pred)\n","    return mse\n","\n","\n","# --- Run optimization with Optuna ---\n","optuna.logging.set_verbosity(optuna.logging.WARNING)\n","study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n","study.optimize(optimize_weight, n_trials=1000, show_progress_bar=True)\n","\n","# --- Results ---\n","best_params = study.best_params\n","best_w_xgb = best_params['xgb_weight']\n","best_w_lgb = best_params['lgb_weight']\n","best_w_rf  = best_params['rf_weight']\n","best_w_cat = best_params['cat_weight']\n","\n","# Normalize again\n","total_weight = best_w_xgb + best_w_lgb + best_w_rf + best_w_cat\n","best_w_xgb /= total_weight\n","best_w_lgb /= total_weight\n","best_w_rf  /= total_weight\n","best_w_cat /= total_weight\n","\n","# --- Final ensemble prediction ---\n","final_pred = (\n","    best_w_xgb * xgb_pred +\n","    best_w_lgb * lgb_pred +\n","    best_w_rf  * rf_pred +\n","    best_w_cat * cat_pred\n",")\n","\n","rmse = np.sqrt(mean_squared_error(y_test, final_pred))\n","\n","# --- Display results ---\n","print(\"\\n=== Optimized Weights ===\")\n","print(f\"XGBoost: {best_w_xgb:.4f}\")\n","print(f\"LightGBM: {best_w_lgb:.4f}\")\n","print(f\"RandomForest: {best_w_rf:.4f}\")\n","print(f\"CatBoost: {best_w_cat:.4f}\")\n","print(f\"\\nFinal RMSE: {rmse:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["xgb_results = evalute_metrics(y_test, xgb_pred).assign(Model=\"XGBoost\")\n","cat_results = evalute_metrics(y_test, cat_pred).assign(Model=\"CatBoost\")\n","lgb_results = evalute_metrics(y_test, lgb_pred).assign(Model=\"LightGBM\")\n","rf_results  = evalute_metrics(y_test, rf_pred).assign(Model=\"RandomForest\")\n","encode_results = evalute_metrics(y_test, final_pred).assign(Model=\"Optimized Ensemble\")\n","\n","# ÁµêÊûú„ÇíÁµêÂêà„Åó„Å¶Ë¶ã„ÇÑ„Åô„ÅèË°®Á§∫\n","results = pd.concat([xgb_results, cat_results, lgb_results, rf_results], ignore_index=True)\n","results = results[['Model', 'MSE', 'MAE', 'R2']]\n","\n","display(results.sort_values('MSE'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:39:03.362959Z","iopub.status.busy":"2025-10-19T11:39:03.362641Z","iopub.status.idle":"2025-10-19T11:39:03.374197Z","shell.execute_reply":"2025-10-19T11:39:03.373478Z","shell.execute_reply.started":"2025-10-19T11:39:03.362934Z"},"trusted":true},"outputs":[],"source":["results = pd.concat(\n","    [xgb_results, lgb_results, rf_results, cat_results, ensemble_results],\n","    ignore_index=True\n",")\n","results = results[['Model', 'MSE', 'MAE', 'R2']]\n","display(results)"]},{"cell_type":"markdown","metadata":{},"source":["# üîç Analysis\n","- All four tree-based models show **very similar performance**, with MSE around `0.00316` and R¬≤ around `0.885`.  \n","- Among single models, **LightGBM** slightly outperforms others in terms of R¬≤.  \n","- The **ensemble model** achieves the **lowest MSE and highest R¬≤**, confirming that combining models improves overall stability and accuracy.  \n","- This demonstrates the benefit of **ensemble learning**, even when base models perform similarly."]},{"cell_type":"markdown","metadata":{},"source":["# submit"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T11:42:16.462992Z","iopub.status.busy":"2025-10-19T11:42:16.462676Z","iopub.status.idle":"2025-10-19T11:42:21.609846Z","shell.execute_reply":"2025-10-19T11:42:21.608673Z","shell.execute_reply.started":"2025-10-19T11:42:16.462971Z"},"trusted":true},"outputs":[],"source":["xgb_pred_new = xgb_model.predict(predict_X)\n","\n","lgb_pred_new = lgb_model.predict(predict_X)\n","\n","rf_pred_new = rf_model.predict(predict_X)\n","\n","cat_pred_new = cat_model.predict(predict_X)\n","\n","final_pred_new = (best_w_xgb * xgb_pred_new \n","                  + best_w_lgb * lgb_pred_new \n","                  + best_w_rf * rf_pred_new\n","                  + best_w_cat * cat_pred_new \n","                  )\n","\n","predict_df = pd.DataFrame(final_pred_new, columns=['accident_risk'])\n","submission = pd.concat([predict['id'], predict_df], axis=1)\n","\n","display(submission.head())\n","print(submission.isnull().sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-19T12:03:37.482455Z","iopub.status.busy":"2025-10-19T12:03:37.482142Z","iopub.status.idle":"2025-10-19T12:03:37.896491Z","shell.execute_reply":"2025-10-19T12:03:37.895649Z","shell.execute_reply.started":"2025-10-19T12:03:37.482436Z"},"trusted":true},"outputs":[],"source":["# --- Save to CSV for Kaggle submission ---\n","submission.to_csv('submission.csv', index=False)\n","print(\"\\n‚úÖ Submission file saved as 'submission.csv'\")"]},{"cell_type":"markdown","metadata":{},"source":["# üèÅ Conclusion\n","\n","In this notebook, we compared four **tree-based regression models** ‚Äî  \n","**XGBoost**, **LightGBM**, **CatBoost**, and **Random Forest** ‚Äî to evaluate their predictive performance on the given dataset.\n","\n","Each model achieved **comparable accuracy**, with **LightGBM** performing slightly better among the individual models.\n","\n","By applying **Optuna-based weight optimization**, we constructed a **weighted ensemble model** that achieved a **marginal yet consistent improvement** in overall performance ‚Äî  \n","reaching the **lowest MSE (0.003152)** and **highest R¬≤ (0.885845)** among all approaches.\n","\n","---\n","\n","### ‚úÖ Key Takeaways\n","- Even when single models perform similarly, **ensemble learning** can capture **complementary strengths**.  \n","- **Proper hyperparameter tuning** and **balanced weighting** lead to more **robust generalization**.  \n","- **Tree-based models** continue to be strong baselines for **structured tabular data** tasks.\n","\n","---\n","\n","### üöÄ Future Directions\n","For further improvement, incorporating **neural network‚Äìbased features** or exploring **meta-model stacking** could enhance predictive accuracy beyond the current ensemble approach.\n","\n","---\n","\n","‚ú® *Overall, this experiment highlights the power of ensemble learning and the importance of model optimization in achieving consistent, high-quality predictions.*\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":13760552,"sourceId":91721,"sourceType":"competition"}],"dockerImageVersionId":31153,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":4}
